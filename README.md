# GPT and Llama Miniatures in Plain Python

This repository is dedicated to the development of miniature, trainable versions of GPT (Generative Pretrained Transformer). It's a clean and plain Python implementation, aiming at educational and experimental purposes primarily. My work draws significant inspiration from Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT/blob/master/model.py) and the LLama project from [Lightning AI](https://github.com/Lightning-AI/lit-llama/tree/main).

## Features

1. In progress pure Python implementation of GPT and LLama LLMs, ideal for those who want to understand the workings of these advanced models without the complexity of huge codebases or library dependencies.
2. In-progress creation of training scripts for ChatGPT and LLama. These versions are being developed to help you create a smaller, yet potent AI model that can be trained on one or more GPUs.
3. Simple and clean code, heavily commented to help you understand every bit of the process.

## Prerequisites

This project assumes you have a basic understanding of Python and the following libraries:

- numpy
- torch

You can install the requirements using pip:

```bash
pip install -r requirements.txt
```


**Note:** The models are still in development. If you want to contribute or experiment with the existing code, feel free to clone the repository.

## Contributing

I welcome contributions to this project. Please feel free to submit issues and pull requests.

## Acknowledgements

Andrej Karpathy for his inspiring work on nanoGPT and Lightning AI for their work on the lit-llama project. These projects have served as an invaluable resource and starting point for this project.

## License

This project is licensed under the MIT License. 

---
